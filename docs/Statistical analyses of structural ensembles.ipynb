{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='toc'> </a> Table of contents\n",
    "\n",
    "- [**Introduction**](#intro)\n",
    "- [**Methods**](#methods)\n",
    "    - [Theory](#methods_theory)\n",
    "        - [Principal Component Analysis](#methods_theory-pca)\n",
    "        - [Independent Component Analysis](#methods_theory-ica)\n",
    "        - [Ward clustering with an informed distance](#methods_theory-clustering)\n",
    "    - [Workflow](#methods_workflow)\n",
    "- [**Results**](#results)\n",
    "    - [RNA Polymerase II](#results_rnapol2)\n",
    "        - [*S.cerevisae*](#results_rnapol2-scerevisae)\n",
    "    - [Ribosome](#results_ribosome)\n",
    "        - [*T.Thermophilus*](#results_ribosome-tthermophilus)\n",
    "            - [Small subunit](#results_ribosome-tthermophilus-ssu)\n",
    "            - [Assembled](#results_ribosome-tthermophilus-assembled)\n",
    "        - [*Mammal*](#results_ribosome-mammal)\n",
    "            - [Small subunit](#results_ribosome-mammal-ssu)\n",
    "            - [Assembled](#results_ribosome-mammal-assembled)\n",
    "- [**References**](#refs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='intro'></a> Introduction\n",
    "[ go back to [TOC](#toc) ]\n",
    "\n",
    "Given a set of structures, our aim is to summarize them in terms of identifying conformational clusters that share a structural similarity together with a *meaningful* description of the space in which these clusters lie. \n",
    "\n",
    "In everyday life terms, imagine that you have access to images of yourself taken throughout the day when you were either standing or sitting, and you want to summarize your activity. A successful summary will be achieved when you will have sorted the images in the \"standing\" or \"sitting\" category/cluster, and will have defined the arrow that join those two clusters, namely the movie interpolating between you sitted and you standing. Each image can be be projected back along this hypothetical but meaningful movie, and the value of its projection is its coordinate along the \"standing-up coordinate\". The resulting coordinates will likely not be evenly distributed and some structures or groups will arise, allowing the categories to be separated. \n",
    "\n",
    "Now suppose that in addition to these two categories, your arms were adopting distinct positions, either extended or not, in a manner entirely independent from the particular state of the body. We would like our analysis to reflect both the \"body standing up\" and the \"arms extending\" movies, and to identify the relevant categories for both consequently. \n",
    "\n",
    "From the initial set of images, the desired method should thus give us i) two independent movies where the body oscillates between the sitted and standing positions for the first one, and where the arms oscillate between the resting and extending positions for the other one, and ii) the categories or clusters that can be identified from the population of the images projected on each movie.\n",
    "\n",
    "Back to our structures, we want to solve the similar problem of identifying relevant \"modes\" that capture independent and relevant directions in the conformational space sampled, together with states that are distinctively populated along these modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='methods'> </a> Methods\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='methods_theory'> </a> Theory\n",
    "[ go back to [TOC](#toc) ]\n",
    "\n",
    "Consider a set of structures (the *samples*) each made of the same $M$ atoms, and store their cartesian coordinates (the *features*) in the $M \\times N$ matrix $\\mathbf{X}$. Below, we introduce a few concepts before outlining the workflow carried out.\n",
    "\n",
    "### <a name='methods_theory-pca'> </a> Principal Component Analysis\n",
    "\n",
    "#### Centering\n",
    "The matrix $\\mathbf{X}$ represents $M$ realization of a random vector $\\mathbf{x}$ of *dimension* $N$. In the following, expectation values of any function $Q$ of this random vector is taken as the sample mean: $E(Q(\\mathbf{x})) = \\frac{1}{M}\\sum{k}^{M}Q(X_{k})$ where $X_{k}$ is the $k$th vector of $\\mathbf{X}$. We start by centering ($aka$ de-meaning) $\\mathbf{X} \\leftarrow \\mathbf{X} - E(\\mathbf{x})$, $i.e.$ subtracting the sample mean from each column vector, resulting in each row of $\\mathbf{X}$ centered on zero.\n",
    "\n",
    "#### Covariance analysis\n",
    "After having demeaned $\\mathbf{X}$, the covariance matrix $\\mathbf{C} = E(\\mathbf{x}\\mathbf{x}^{T})$ can be estimated from the sample resulting in a $N\\times N$ positive semi-definite matrix with $M$ eigenvectors $\\mathbf{U}$ of dimension $N$ and associated eigenvalues stored in the $N\\times N$ diagonal matrix $\\mathbf{\\Sigma}$:\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{C} &\\propto& \\mathbf{X}\\mathbf{X}^{T} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{U}^{T}\\\\\\nonumber\n",
    "C_{ij}&=&\\sum_{k}^{M}\\sigma_{k}U_{k}(i)U_{k}(j)\n",
    "\\end{eqnarray}\n",
    "The resulting eigenvectors can be used as a basis for $\\mathbf{x}=\\sum_{k}^{M}(\\mathbf{U}_{k}^{T}\\mathbf{x})\\mathbf{U}_{k}$, and the full principal component decomposition of $\\mathbf{X}$ is given by the $M\\times M$ score matrix $\\mathbf{T} = \\mathbf{U}^{T}\\mathbf{X}$, as in return $X_{i} = \\sum_{k}^{M}T_{ki}U_{k}$.\n",
    "\n",
    "#### Singular value (polar) decomposition \n",
    "Alternatively, the singular value decomposition of $\\mathbf{X}$ yields left eigenvectors that coincide with the principal components $\\mathbf{U}$, and associated eigenvalue that are stored in a diagonal matrix $\\mathbf{\\Lambda}$ such that $\\mathbf{\\Lambda}\\mathbf{\\Lambda}^{T}=\\mathbf{\\Sigma}$, and we get an additional unitary $M\\times M$ matrix $\\mathbf{V}$. \n",
    "\\begin{eqnarray}\\label{svd}\n",
    "\\mathbf{X}=\\mathbf{U}\\mathbf{\\Lambda}\\mathbf{V}^{T}\n",
    "\\end{eqnarray}\n",
    "This also yields the polar decomposition of the score matrix $\\mathbf{T}=\\mathbf{\\Lambda}\\mathbf{V}^{T}$, illustrating why $\\mathbf{U}$ is sometimes referred to as the whitening or sphering matrix.\n",
    "The coordinate $T_{k}(i)$ of the $i$-th sample of $\\mathbf{X}$ along the $k$-th dimension of $\\mathbf{U}$-space is $T_{ki}=\\lambda_{k}V_{k}(i)$.\n",
    "\n",
    "Interestingly, if the eigenvalues are sorted in decreasing order, we can truncate the basis to order $L\\leq M$ such that  $\\sum^{L} \\lambda^{2}_{k} \\approx \\sum^{M} \\lambda^{2}_{k} $. This still yields a good estimate of the covariance of $\\mathbf{x}$: $\\mathbf{C} \\approx \\mathbf{U}_{L}\\mathbf{\\Lambda}_{L}\\mathbf{U}_{L}^{T}$. The truncated principal decomposition of $\\mathbf{X}$ thus becomes the $L\\times M$ matrix $\\mathbf{T}_{L} = \\mathbf{U}_{L}^{T}\\mathbf{X} = \\mathbf{\\Lambda}_{L}\\mathbf{V}_{L}^{T}$.\n",
    "\n",
    "By truncating to order $L<<N$, one often still accounts for most of the variability in the data while drastically reducing its dimensionality.\n",
    "\n",
    "### <a name='methods_theory-ica'> </a> Independent Component Analysis\n",
    "\n",
    "#### The mixture problem\n",
    "Let consider the observed random vector $\\mathbf{x}$ as a mixture of *latent variables* $\\mathbf{s}$ of unknown distribution. We note $\\mathbf{A}$ the unknown mixing matrix that relates them. We are interested in solving this blind source separation problem and find $\\mathbf{s}$ by finding the unmixing matrix $\\mathbf{M}$ such that:\n",
    "\\begin{eqnarray}\\label{mixpb}\n",
    "\\mathbf{x} &=& \\mathbf{A}\\mathbf{s}\\\\\\nonumber\n",
    "\\mathbf{s} &=& \\mathbf{M}\\mathbf{x}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Independence and nongaussianity\n",
    "The Central Limit Theorem states that the distribution of a sum of independent random variables is usually more gaussian than that of the original random variables. Solving our mixture problem thus consists in looking for independent latent variables with non-gaussian distributions, or equivalently to find the random row vectors $\\mathbf{m}^{T}$ of $\\mathbf{M}$ that maximizes the non-gaussianity of $\\mathbf{m}^{T}\\mathbf{x}=\\mathbf{z}^{T}\\mathbf{s}$, with $\\mathbf{m}^{T}\\mathbf{A} = \\mathbf{z}^{T}$, which is achieved when $\\mathbf{z}$ has only one non-zero component.\n",
    "\n",
    "*Nongaussianity* of a random variable is measured by the negentropy $J$, defined as the differential entropy $H$ of the variable subtracted from that of a gaussian random variable of same covariance. $J$ is always non-negative and is zero only if the random variable is gaussian. Noting $f$ the joint density of $\\mathbf{s}$:\n",
    "\\begin{eqnarray}\n",
    "H(\\mathbf{s}) &=& -\\int d\\mathbf{s} f(\\mathbf{s})\\log f(\\mathbf{s})= -E\\big(\\log f(\\mathbf{s})\\big)\\\\\\nonumber\n",
    "J(\\mathbf{s}) &=& H(\\mathbf{s}_{gauss}) - H(\\mathbf{s})\n",
    "\\end{eqnarray}\n",
    "\n",
    "*Independence* means that the joint probability is equal to the product of the individual probabilities. It is measured by their mutual information $I$:\n",
    "\\begin{eqnarray}\n",
    "I(\\mathbf{s}) = \\sum_{i}H(s_i) - H(\\mathbf{s})\n",
    "\\end{eqnarray}\n",
    "\n",
    "Independence implies uncorrelatedness, and if we further constrained $\\mathbf{s}$ to be of unit variance, we can show that *independent is nongaussian*:\n",
    "\\begin{eqnarray}\n",
    "I(\\mathbf{s}) &=& C - \\sum_{i}J(s_{i})\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Whitening\n",
    "As we just saw, we are looking for uncorrelated components $\\mathbf{s}$ of unit variance, so we can write $\\mathbf{C}=\\mathbf{A}\\mathbf{A}^{T}$. If we had \"whitened\" the data so its covariance is $\\mathbf{I}$, we see that $\\mathbf{A}$ becomes orthogonal, which already reduces greatly the complexity of finding it. Whitening  $\\mathbf{X}$  of covariance  $\\mathbf{C}$  means that we apply the whitening matrix  $\\mathbf{W}$  to it, resulting in  $\\mathbf{Y}=\\mathbf{W}\\mathbf{X}$  of unit diagonal covariance. It is enough for  $\\mathbf{W}$  to satisfy $\\mathbf{W}^{T}\\mathbf{W}=\\mathbf{C}^{-1}$, hence it is not defined uniquely. Several approaches can thus be used, such as Mahalanobis or PCA whitening, where the former has been shown to maintain a good correlation between the initial and the whitened data, while the latter tends to compress the initial data into the first white components (see [Kessy et al.](#ref_kessy)). The latter feature is desirable in terms of computational efficiency, and we choose it. It amounts to performing the PCA (see section above) and defining the $L\\times N$ matrix $\\mathbf{W}$ and compute the  $L\\times M$ matrix $\\mathbf{Y}$:\n",
    "\\begin{eqnarray}\\label{whitening}\n",
    "\\mathbf{W}\n",
    "&=& \\mathbf{\\Sigma}_{L}^{-1}\\mathbf{U}_{L}^{T}\\\\\\nonumber\n",
    "\\mathbf{Y}\n",
    "&=& \\mathbf{V}_{L}^{T}\n",
    "\\end{eqnarray}\n",
    "\n",
    "In other words, the normalized coordinates of the data in the orthonormal basis defined by its first $L$ principal components is what we will consider now.\n",
    "\n",
    "#### fastICA\n",
    "We use the fastICA algorithm [Hyvarinen](#ref_hyvarinen) to perform the unmixing by maximizing the negentropy of the estimated components. Given a set of nonquadratic function $G_{n}$ and scalars $k_{n}$, the negentropy $J$ of the current component $p$ estimated by projecting the whitened data with the unmixing vector $\\mathbf{m}_{p}^{T}$ can be approximated as :\n",
    "\\begin{eqnarray}\n",
    "J(\\mathbf{s}_{p}) \n",
    "&\\approx& \\sum_{n}k_{n}\\big[E\\big(G_{n}(\\mathbf{m}_{p}^{T}\\mathbf{y})\\big) - E\\big(G_{n}(\\mathbf{m}_{p}^{T}\\mathbf{y}_{gauss})\\big)\\big]^{2}\n",
    "\\end{eqnarray}\n",
    "To maximize it, it is enough that for all $n$ we maximize $E\\big(G_{n}(\\mathbf{m}_{p}^{T}\\mathbf{y})\\big)$ under the constraint $E\\big((\\mathbf{m}_{p}^{T}\\mathbf{y})^{2}\\big)=1$. For $n=1$, noting $g$ and $g'$ the first and second derivative of $G$, this amounts (ref) to finding a root to $F$ defined below together with its jacobian $JF$:\n",
    "\\begin{eqnarray}\n",
    "F(\\mathbf{m}_{p})\n",
    "= E\\big(\\mathbf{y}g(\\mathbf{m}_{p}^{T}\\mathbf{y})\\big) - \\beta \\mathbf{m}_{p}\n",
    "\\text{, and}\n",
    "JF(\\mathbf{m}_{p})\n",
    "\\approx \\Big(E\\big(g'(\\mathbf{m}_{p}^{T}\\mathbf{y})\\big) - \\beta\\Big)\\mathbf{I}\n",
    "\\end{eqnarray}\n",
    "With these quantities derived, the fastICA algorithm can be seen as an approximative Newton iterative scheme, where, starting from a random vector $\\mathbf{m}_{p}^{T}$, it is iteratively updated and normalized until convergence. The update and convergence criterion are given below:\n",
    "\\begin{eqnarray}\n",
    "&\\bullet &\n",
    "~~ \\mathbf{m}^{+} \n",
    "= \\frac{1}{M}\\mathbf{Y}g(\\mathbf{m}_{p}^{T}\\mathbf{Y})^{T} - \\frac{1}{M}\\big(g'(\\mathbf{m}_{p}^{T}\\mathbf{Y})\\mathbf{1}\\big)\\mathbf{m}_{p}\\\\\\nonumber\n",
    "&&\\text{then}\\mathbf{m}_{p} \n",
    "\\leftarrow  \\mathbf{m}^{+}/|\\mathbf{m}^{+}|\\\\\\nonumber\n",
    "&\\bullet&\n",
    "~~ \\mathbf{m}_{p}^{T}\\mathbf{m}^{(old)}_{p} \\approx  1\n",
    "\\end{eqnarray}\n",
    "An additional step is necessary to decorrelate added components. We implemented a Gram-Schmidt-like step but other approaches could be tried. Namely\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{m}_{k+1} \n",
    "&\\leftarrow& \\mathbf{m}_{k+1} - \\sum_{i}^{k} (\\mathbf{m}_{k+1}^{T}\\mathbf{m}_{i})\\mathbf{m}_{i}\\\\\\nonumber\n",
    "&&\\text{then}\\mathbf{m}_{k+1}\\leftarrow \\mathbf{m}_{k+1}/|\\mathbf{m}_{k+1}|\n",
    "\\end{eqnarray}\n",
    "\n",
    "After $C$ unmixing vectors have been found, we get the $C\\times M$ independent components matrix $\\mathbf{S}$ where the $p$-th row vector is given below, together with its estimated negentropy:\n",
    "\\begin{eqnarray}\n",
    "\\forall i\\in (1,M), \\mathbf{s}_{p}(i) &=& \\mathbf{m}_{p}^{T}Y_{i} = \\sum_{k}^{L}m_{p}(k)v_{k}(i)\\\\\\nonumber\n",
    "J(\\mathbf{s}_{p}) &=& \\big[\\frac{1}{M}\\big(G(\\mathbf{m}_{p}^{T}\\mathbf{Y}) - G(\\mathbf{m}_{p}^{T}\\mathbf{Y}_{gauss})\\big)\\mathbf{1}\\big]^{2}\n",
    "\\end{eqnarray}\n",
    "It remains to be seen if we can further reduce the dimensionality of the problem by keeping only the components with higher negentropy.\n",
    "\n",
    "#### New coordinates\n",
    "\n",
    "After performing PCA and ICA, we see that we can write our structural set on two different basis sets:\n",
    "\\begin{eqnarray}\n",
    "X_{j}(i) &=& \\sum_{k=1}^{L}v_{jk}P_{k}(i)\\text{where}P_{k}(i)=\\sigma_{k}U_{k}(i)\\\\\\nonumber\n",
    "&=& \\sum_{k=1}^{C}s_{kj}Q_{k}(i)\\text{where}Q_{k}(i)=\\sum_{l=1}^{L}m_{lk}^{-1}P_{l}(i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that while the $U_{k}$ formed an orthonormal basis of the conformational space in the case of principal component analysis, we see here that in the case of independent component analysis the $Q_{k}$ are not necessarily orthonormal to one another. Rather, they point to directions in the conformational space where the projection of the data is maximally relevant.\n",
    "\n",
    "### <a name='methods_theory-clustering'> </a>Ward clustering with an informed distance\n",
    "\n",
    "We introduce the \"informed distance\" between two structures $i$ and $j$ as:\n",
    "\\begin{eqnarray}\n",
    "d_{ij} = \\sqrt{\\sum_{k}^{L}J(\\mathbf{s}_{k})\\big(s_{kj}-s_{ki}\\big)^{2}}\n",
    "\\end{eqnarray}\n",
    "and use it to perform Ward clustering of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='methods_workflow'> </a> Workflow\n",
    "[ go back to [TOC](#toc) ]\n",
    "\n",
    "Starting from the whole centered dataset, PCA is carried out, and truncated to a given $\\epsilon$ (*default:* 0.75), thus yielding the whitened input to the subsequent ICA, yielding the coordinates of the data in the new IC-space (the sources). The sources are then hierarchically clustered, and the resulting dendogram is visually inspected to decide on the number of clusters to choose. The process is then repeated iteratively on each cluster (recentered on itself), until no source has any significant negentropy remaining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='results'> </a>Results\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='results_rnapol2'> </a> RNA Polymerase II\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='results_rnapol2-scerevisae'> </a> *S.cerevisae*\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='results_ribosome'> </a>Ribosome\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='results_ribosome-tthermo'> </a> *T.Thermophilus*\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='results_ribosome-tthermo-ssu'> </a> Small subunit\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='results_ribosome-tthermo-assembled'> </a>Assembled\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='results_ribosome-mammal'> </a> Mammal \n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='results_ribosome-mammal-ssu'> </a>Small subunit\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='results_ribosome-mammal-assembled'> </a> Assembled\n",
    "[ go back to [TOC](#toc) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='refs'></a> References\n",
    "[ go back to [TOC](#toc) ]\n",
    "\n",
    "1. <a name=\"ref_kessy\">Kessy et al</a>\n",
    "2. <a name=\"ref_hyvarinen\"> Hyvarinen et al</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
